{
  "safety_risk": {
    "id": "safety_risk",
    "human_name_en": "Safety & Risk Information",
    "governance_axis": "safety",
    "importance_weight": 0.95,
    "description_en": "Information about safety evaluations, known dangerous capabilities, refusal behaviour, incidents, red-teaming results, and risk-specific benchmarks for the AI system.",
    "examples": [
      "Reported jailbreak / attack success rates under different prompts or threat models.",
      "Explicit descriptions of dangerous capabilities (e.g., generating phishing emails, helping with malware, or enabling fraud).",
      "Refusal rates by category (e.g., CBRN, hate, self-harm) and how they were measured.",
      "Post-deployment incident reports with dates, severity, and remediation status.",
      "Safety evaluation benchmark scores (toxicity, red-teaming suites, hallucination tests) and how representative they are."
    ],
    "keywords": [
      "jailbreak",
      "attack",
      "adversarial",
      "phishing",
      "exploit",
      "refusal",
      "decline",
      "reject",
      "rejected",
      "blocked",
      "incident",
      "misuse",
      "abuse",
      "safety evaluation",
      "safety benchmark",
      "toxicity",
      "toxic",
      "harassment",
      "dangerous",
      "risk",
      "hazard",
      "red teaming",
      "red-teaming",
      "threat model",
      "threat modeling",
      "CBRN",
      "chemical",
      "biological",
      "radiological",
      "nuclear",
      "weapons",
      "malware",
      "cyberattack",
      "fraud",
      "deception",
      "misinformation",
      "disinformation",
      "hallucination",
      "self-harm",
      "suicide",
      "violence",
      "hate speech",
      "illegal activity",
      "security",
      "vulnerability"
    ],
    "rubric_links": {
      "mit_pitch": [
        "Technicality – evidence-backed statements about safety.",
        "Equity – documentation of harms and differential risks for vulnerable groups.",
        "Viability – realistic description of remaining risks."
      ],
      "mit_memo": [
        "Data Analysis – safety metrics are clearly explained for non-experts.",
        "Trade-Offs – explicit discussion of remaining uncertainties and limitations."
      ]
    },
    "question_templates_en": [
      "What are the main dangerous capabilities of this system, and how are they measured?",
      "What red-teaming or adversarial testing has been conducted, and with what results?",
      "How are safety incidents recorded, investigated, and reported?",
      "What is the refusal rate for different categories of harmful requests?",
      "How does the system handle attempts to bypass safety measures?"
    ]
  },
  "intended_use_limitations": {
    "id": "intended_use_limitations",
    "human_name_en": "Intended Use & Limitations",
    "governance_axis": "scope",
    "importance_weight": 0.9,
    "description_en": "What the system is designed and approved to be used for, which uses are explicitly out-of-scope, and what limitations are known in terms of domain, context, or population.",
    "examples": [
      "The system is intended for customer support summarization, not for medical triage or diagnosis.",
      "Out-of-scope use cases are clearly listed (e.g., critical infrastructure, weapons design, medical emergencies).",
      "Known limitations regarding legal advice, high-stakes decision-making, or use by children.",
      "Context window length limits and how quality degrades with very long prompts or conversations."
    ],
    "keywords": [
      "intended use",
      "intended usage",
      "primary use",
      "use case",
      "use cases",
      "scope",
      "out-of-scope",
      "out of scope",
      "not designed",
      "not intended",
      "limitations",
      "limitation",
      "known issues",
      "assumptions",
      "constraints",
      "context window",
      "token limit",
      "long context",
      "not recommended",
      "should not",
      "avoid using",
      "inappropriate",
      "unsuitable",
      "disclaimer",
      "warning",
      "caution",
      "high-stakes",
      "critical infrastructure",
      "medical",
      "legal",
      "financial advice",
      "children",
      "minors"
    ],
    "rubric_links": {
      "mit_pitch": [
        "Viability – realistic and bounded scope of deployment.",
        "Presentation Quality – clear explanation of where the solution should or should not be used."
      ],
      "mit_memo": [
        "Clarity – the memo clearly states recommended and forbidden uses.",
        "Recommendations – procurement or policy guidance aligned with intended scope."
      ]
    },
    "question_templates_en": [
      "What is this system explicitly designed for, and what uses are out-of-scope?",
      "In which domains or decision contexts should this model never be used?",
      "How does performance degrade outside the intended domain or beyond the context window?",
      "What warnings or disclaimers are provided to users about limitations?",
      "Are there specific populations or contexts where this system should not be deployed?"
    ]
  },
  "training_data": {
    "id": "training_data",
    "human_name_en": "Training & Data",
    "governance_axis": "data",
    "importance_weight": 0.9,
    "description_en": "Sources, time ranges, curation, filtering, licensing, and synthetic data usage for training and fine-tuning the model or system.",
    "examples": [
      "Training data sourced from web crawl up to June 2024, with specific exclusions (e.g., certain sites or content types).",
      "High-level breakdown of data domains (code, natural language, images, speech) and languages.",
      "Details on data filtering, deduplication, and content moderation applied pre-training.",
      "Percentage and role of synthetic data (self-play, distilled, augmented) in pre-training or fine-tuning.",
      "Licensing status of key data sources or datasets."
    ],
    "keywords": [
      "training data",
      "pre-training data",
      "pretraining data",
      "dataset",
      "datasets",
      "corpus",
      "corpora",
      "data source",
      "data sources",
      "data collection",
      "web data",
      "web crawl",
      "scraped",
      "scraping",
      "filtering",
      "filter",
      "filtered",
      "curation",
      "curated",
      "deduplication",
      "deduplicated",
      "synthetic data",
      "generated data",
      "augmentation",
      "augmented",
      "license",
      "licensing",
      "licensed",
      "copyright",
      "dataset card",
      "data card",
      "datasheet",
      "fine-tuning data",
      "RLHF",
      "reinforcement learning",
      "human feedback",
      "annotation",
      "annotated",
      "labeling",
      "labeled",
      "data composition",
      "data mix",
      "cutoff date",
      "time range",
      "languages",
      "multilingual"
    ],
    "rubric_links": {
      "mit_pitch": [
        "Technicality – transparent statement of what data underpins the system.",
        "Equity – awareness of gaps or skews in training data that affect marginalized groups."
      ],
      "mit_memo": [
        "Data Analysis – explanation of how data composition may affect behaviour.",
        "Trade-Offs – balancing privacy, coverage, and representativeness in training data choices."
      ]
    },
    "question_templates_en": [
      "From which sources and time periods does the training data come?",
      "How were data filtered, deduplicated, and moderated before training?",
      "What proportion of the training data is synthetic, and for which purposes is it used?",
      "What is the demographic and linguistic composition of the training data?",
      "What licensing constraints exist on the training data?",
      "How was human feedback or RLHF data collected and annotated?"
    ]
  },
  "performance_capabilities": {
    "id": "performance_capabilities",
    "human_name_en": "Performance & Capabilities",
    "governance_axis": "capabilities",
    "importance_weight": 0.85,
    "description_en": "Benchmark performance, emergent abilities, and domain-specific strengths and weaknesses, including where the model should not be trusted.",
    "examples": [
      "Scores on standard benchmarks (MMLU, HumanEval, GSM8K, translation tasks) with caveats.",
      "Comparisons to previous versions (e.g., 20% improvement in coding tasks, degraded performance on rare languages).",
      "Domain-specific performance breakdowns (law, medicine, education, creative writing).",
      "Explicit statements about tasks where performance is known to be unreliable."
    ],
    "keywords": [
      "benchmark",
      "benchmarks",
      "MMLU",
      "HumanEval",
      "GSM8K",
      "MATH",
      "HellaSwag",
      "ARC",
      "TruthfulQA",
      "accuracy",
      "precision",
      "recall",
      "F1",
      "F1 score",
      "performance",
      "capability",
      "capabilities",
      "improvement",
      "increase",
      "delta",
      "evaluation",
      "evaluated",
      "score",
      "scores",
      "results",
      "domain-specific",
      "task-specific",
      "legal",
      "medical",
      "coding",
      "code generation",
      "programming",
      "math",
      "mathematics",
      "reasoning",
      "translation",
      "multilingual",
      "creative writing",
      "summarization",
      "question answering",
      "baseline",
      "comparison",
      "state-of-the-art",
      "SOTA",
      "pass rate",
      "error rate",
      "latency",
      "throughput",
      "emergent"
    ],
    "rubric_links": {
      "mit_pitch": [
        "Technicality – grounded performance claims backed by concrete benchmarks.",
        "Originality – framing capabilities in ways that are useful for governance and policy."
      ],
      "mit_memo": [
        "Clarity – explanation of what the model can and cannot do, in non-technical language.",
        "Recommendations – aligning deployment suggestions with demonstrated capabilities."
      ]
    },
    "question_templates_en": [
      "On which benchmarks has this system been evaluated, and how do the scores compare to baselines?",
      "In which domains should the model be considered strong, and where should it not be trusted?",
      "How do capabilities differ from previous versions or neighbouring models?",
      "What are the known failure modes or task categories where performance is poor?",
      "How does performance vary across different languages or cultural contexts?"
    ]
  },
  "organizational_governance": {
    "id": "organizational_governance",
    "human_name_en": "Organizational & Governance",
    "governance_axis": "governance",
    "importance_weight": 0.9,
    "description_en": "Information about the development process, internal oversight, external audits, update policies, roles and responsibilities, and user appeals mechanisms.",
    "examples": [
      "Overview of the development timeline, team composition, and responsible AI processes.",
      "Description of governance structures (review boards, ethics committees, red-teaming groups).",
      "Third-party audits or evaluations (by whom, with what access, with what limitations).",
      "Model update and deprecation policies, including communication strategies for users.",
      "Appeals processes for harmful outputs or incorrect refusals."
    ],
    "keywords": [
      "governance",
      "oversight",
      "board",
      "committee",
      "responsible AI",
      "responsible-ai",
      "responsible development",
      "audit",
      "auditor",
      "auditing",
      "third-party",
      "third party",
      "external evaluation",
      "external review",
      "independent",
      "update policy",
      "updates",
      "versioning",
      "deprecation",
      "deprecated",
      "lifecycle",
      "development process",
      "risk committee",
      "ethics review",
      "ethics board",
      "appeal",
      "appeals",
      "complaint",
      "complaints",
      "escalation",
      "incident response",
      "incident management",
      "transparency",
      "accountability",
      "stakeholder",
      "consultation",
      "review process",
      "approval process",
      "decision-making",
      "rollout",
      "deployment timeline"
    ],
    "rubric_links": {
      "mit_pitch": [
        "Viability – realistic institutional arrangements for ongoing governance.",
        "Equity – accountability pathways for affected communities."
      ],
      "mit_memo": [
        "Recommendations – governance mechanisms that policymakers or procurers can require.",
        "Trade-Offs – acknowledging resource and capacity constraints in governance design."
      ]
    },
    "question_templates_en": [
      "Which governance structures and review processes oversee this system through its lifecycle?",
      "Has the system undergone external audits or evaluations? By whom and with what access?",
      "What are the update, rollback, and deprecation policies for this model?",
      "How are decisions about model changes and deployments made?",
      "What mechanisms exist for users to report issues or appeal decisions?"
    ]
  },
  "access_deployment": {
    "id": "access_deployment",
    "human_name_en": "Access & Deployment",
    "governance_axis": "deployment",
    "importance_weight": 0.8,
    "description_en": "Who can access the system, under what conditions, how usage is monitored and rate-limited, and what compute or infrastructure is required for safe deployment.",
    "examples": [
      "Access restrictions (e.g., consumer vs. developer vs. enterprise tiers).",
      "Identity verification, KYC, or other checks for high-risk access.",
      "Rate limits and monitoring for abuse, including thresholds that trigger review.",
      "Compute requirements (GPU, memory, latency) for on-prem or regulated settings.",
      "Logging and audit trails retained to support safety monitoring."
    ],
    "keywords": [
      "access",
      "api access",
      "API access",
      "access control",
      "eligibility",
      "verification",
      "KYC",
      "know your customer",
      "identity verification",
      "API keys",
      "API key",
      "rate limit",
      "rate limits",
      "rate-limit",
      "throttling",
      "quota",
      "monitoring",
      "monitored",
      "logging",
      "logs",
      "audit trail",
      "review",
      "flags",
      "flagged",
      "compute",
      "GPU",
      "hardware",
      "infrastructure",
      "deployment",
      "deployed",
      "on-prem",
      "on-premises",
      "cloud",
      "availability",
      "available",
      "tier",
      "tiers",
      "pricing",
      "enterprise",
      "commercial",
      "public",
      "private",
      "restricted",
      "registration",
      "terms of service",
      "acceptable use",
      "usage policy"
    ],
    "rubric_links": {
      "mit_pitch": [
        "Viability – realistic deployment pathways and constraints.",
        "Technicality – clear explanation of deployment and monitoring mechanisms."
      ],
      "mit_memo": [
        "Recommendations – access controls and logging requirements for regulators or procurers.",
        "Trade-Offs – balancing openness, innovation, and safety in access design."
      ]
    },
    "question_templates_en": [
      "Who is allowed to access this system, and under what conditions?",
      "What rate limits, logging, or monitoring are in place to detect misuse?",
      "What infrastructure is required to deploy this system safely in sensitive environments?",
      "What verification or approval processes exist for accessing the system?",
      "How long are usage logs retained, and who has access to them?"
    ]
  },
  "equity_bias": {
    "id": "equity_bias",
    "human_name_en": "Equity & Bias",
    "governance_axis": "equity",
    "importance_weight": 1.0,
    "description_en": "Specific information about demographic biases, subgroup performance, fairness metrics, and known harms or disparities affecting marginalized or intersectional groups.",
    "examples": [
      "Performance metrics disaggregated by race, gender, language, disability status, or geography.",
      "Qualitative accounts of harms experienced by marginalized groups in real deployments.",
      "Fairness metrics used (e.g., demographic parity, equalized odds) and their limitations.",
      "Mitigation strategies (data balancing, debiasing, selective deployment, warnings) with evidence.",
      "Discussion of intersectional risks (e.g., Black women, queer communities, migrants)."
    ],
    "keywords": [
      "bias",
      "biases",
      "biased",
      "fairness",
      "fair",
      "equity",
      "equitable",
      "disparity",
      "disparities",
      "disparate impact",
      "demographic",
      "demographics",
      "subgroup",
      "subgroups",
      "disaggregated",
      "protected group",
      "protected groups",
      "race",
      "racial",
      "ethnicity",
      "ethnic",
      "gender",
      "gendered",
      "women",
      "LGBTQ",
      "LGBTQIA",
      "queer",
      "transgender",
      "non-binary",
      "disability",
      "disabled",
      "accessibility",
      "accessible",
      "marginalized",
      "vulnerable",
      "underrepresented",
      "minority",
      "minorities",
      "harm",
      "harms",
      "harmful",
      "discrimination",
      "discriminatory",
      "stereotype",
      "stereotypes",
      "stereotyping",
      "intersectional",
      "intersectionality",
      "demographic parity",
      "equalized odds",
      "equal opportunity",
      "fairness metric",
      "debiasing",
      "debias",
      "mitigation",
      "mitigate",
      "representation",
      "representational harm",
      "allocative harm",
      "age",
      "geography",
      "socioeconomic",
      "language",
      "dialect",
      "accent"
    ],
    "rubric_links": {
      "mit_pitch": [
        "Equity – central to judging; shows awareness of disparate impacts and mitigation.",
        "Originality – intersectional framing of documentation gaps and harms."
      ],
      "mit_memo": [
        "Equity – explicit integration of fairness and subgroup analysis into recommendations.",
        "Data Analysis – trade-offs and limitations of current fairness practices are explained."
      ]
    },
    "question_templates_en": [
      "How does the system perform across different demographic and intersectional groups?",
      "Which fairness metrics or evaluations have been applied, and what were the results?",
      "What harms or disparities have been identified for marginalized communities, and what mitigations exist?",
      "Are performance metrics disaggregated by protected characteristics?",
      "What representational or allocative harms have been documented?",
      "How does the system handle dialect, accent, or non-standard language variations?"
    ]
  },
  "other": {
    "id": "other",
    "human_name_en": "Other / Uncategorised",
    "governance_axis": "misc",
    "importance_weight": 0.2,
    "description_en": "Content that does not clearly fall into any of the governance-relevant categories above, including marketing language, generic mission statements, or vague claims without evidence.",
    "examples": [
      "High-level marketing language about innovation, leadership, or disruption.",
      "Generic company mission statements without concrete commitments or metrics.",
      "Narrative and storytelling that does not map to specific documentation requirements."
    ],
    "keywords": [
      "innovation leader",
      "cutting-edge",
      "state of the art",
      "market leading",
      "vision",
      "mission",
      "values",
      "brand",
      "story",
      "disruption",
      "disruptive",
      "thought leader",
      "revolutionary",
      "breakthrough",
      "pioneering",
      "world-class",
      "best-in-class"
    ],
    "rubric_links": {
      "mit_pitch": [
        "Helps distinguish between substantive documentation and pure PR.",
        "Supports originality by showing where documentation is mostly marketing."
      ],
      "mit_memo": [
        "Clarity – separates evidence-based claims from vague narratives.",
        "Trade-Offs – notes where transparency is weak or purely performative."
      ]
    },
    "question_templates_en": [
      "Which parts of the documentation are primarily marketing rather than substantive transparency?",
      "Where are there high-level commitments without concrete metrics or mechanisms?",
      "What information is missing that would turn generic claims into accountable commitments?"
    ]
  }
}